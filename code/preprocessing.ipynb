{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08de8485-6875-4597-982a-06f6b81aa83f",
   "metadata": {},
   "source": [
    "### preprocessing  \n",
    "For each {foldername} in ..\\\\data\\\\{train, test, val}, it does preprocess on \\\\{foldername}\\\\openface_asd_pairs.csv  \n",
    "and saves the result into \\\\{foldername}\\\\preprocessed\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ee138b-8aa0-4310-800c-42576d814ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a0d2b1-f3c7-4c7a-aa86-3e779dc08332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('../data/train/train_vimeo_562725349'), WindowsPath('../data/train/train_vimeo_611083383'), WindowsPath('../data/test/test_vimeo_414944373'), WindowsPath('../data/val/val_vimeo_429364395')]\n"
     ]
    }
   ],
   "source": [
    "folder = [Path(\"..\\\\data\\\\train\"),Path(\"..\\\\data\\\\test\"), Path(\"..\\\\data\\\\val\")]\n",
    "subfolder=[]\n",
    "for i in folder:\n",
    "    temp=[i / f for f in os.listdir(i) if not f.startswith('.') and os.path.isdir(i / f)]\n",
    "    subfolder = subfolder + temp\n",
    "\n",
    "# # if test on a single file\n",
    "# subfolder=subfolder[0:1]\n",
    "\n",
    "print(subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3e693b-a1e9-429b-8293-8bd14b72d4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters for preprocessing\n",
    "\n",
    "# step 1 (smoothing), remove short silence intervals that are < PP1_th frames\n",
    "PP1 = True\n",
    "PP1_th = 25\n",
    "\n",
    "# step 2 (smoothing), remove short utterance intervals that are < PP2_th frames\n",
    "PP2 = True\n",
    "PP2_th = 25\n",
    "\n",
    "# step 3 (data sample determine)\n",
    "# for a speaker start speaking at time t, the data sample x is determined by [t-LEN_th_max, t-1-LEN_exclude], and t>=LEN_th_min\n",
    "PP3 = True \n",
    "LEN_th_max = 10*25\n",
    "LEN_th_min = 25\n",
    "LEN_exclude = 'silence' # int or 'silence', skip a length just before the next speaker's utterance to prevent the mouth-openning frame affecting the classifier\n",
    "\n",
    "# step 4 (feature selection)\n",
    "PP4 = True\n",
    "InterpolateNaN = True # interpolation NaNs in the data samples. if 'False', rows including NaNs will be removed.\n",
    "#### feature definition start ####\n",
    "gaze_direction_3d = ['gaze_0_x', 'gaze_0_y', 'gaze_0_z', 'gaze_1_x', 'gaze_1_y', 'gaze_1_z']\n",
    "gaze_direction_2d = ['gaze_angle_x','gaze_angle_y']\n",
    "eye_landmarks_2d = [f'eye_lmk_x_{i}' for i in range(0, 56)] + [f'eye_lmk_y_{j}' for j in range(0, 56)]\n",
    "eye_landmards_3d = [f'eye_lmk_X_{i}' for i in range(0, 56)] + [f'eye_lmk_Y_{j}' for j in range(0, 56)]\n",
    "head_position = ['pose_Tx', 'pose_Ty', 'pose_Tz', 'pose_Rx', 'pose_Ry', 'pose_Rz']\n",
    "face_landmark_2d = [f'x_{i}' for i in range(0, 68)] + [f'y_{j}' for j in range(0, 68)]\n",
    "face_landmark_3d = [f'X_{i}' for i in range(0, 68)] + [f'Y_{j}' for j in range(0, 68)] + [f'Z_{k}' for k in range(0, 68)]\n",
    "PDM_rigid = ['p_scale', 'p_rx', 'p_ry', 'p_rz', 'p_tx', 'p_ty']\n",
    "PDM_nonrigid = [f'p_{i}' for i in range(0, 34)]\n",
    "FAU_intensity = ['AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r',\n",
    "                 'AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r']\n",
    "FAU_presence = ['AU01_c', 'AU02_c', 'AU04_c', 'AU05_c', 'AU06_c', 'AU07_c', 'AU09_c', 'AU10_c', 'AU12_c',\n",
    "                'AU14_c', 'AU15_c', 'AU17_c', 'AU20_c', 'AU23_c', 'AU25_c', 'AU26_c', 'AU28_c', 'AU45_r']\n",
    "info_col = ['frame','bbox_xmin','bbox_ymin','bbox_xmax','bbox_ymax'] # frame_id and bbox_ASD's position\n",
    "#### feature definition end ####\n",
    "output_col = gaze_direction_2d + FAU_intensity # this line selects features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f823d-944a-45ef-a1d7-e9ff2886819b",
   "metadata": {},
   "source": [
    "Load openface_asd_pairs.csv and create turn.csv [frame, [speaker_id], [listener_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754ea26f-3d4f-4009-a98d-8bfd77218bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = 'openface_asd_pairs.csv'\n",
    "output_file = \"turn.csv\"\n",
    "output_folder = \"preprocessed\"\n",
    "\n",
    "for i in subfolder:\n",
    "    df = pd.read_csv(i / input_file)\n",
    "    # select rows of the same frame\n",
    "    # define turn_speaker = [track_id1, track_id2, ...] for all the track_id whose score>0, or =[] if no track_id has score>0\n",
    "    # make [\"frame\" \"speaker\" \"listener\"] as a new dataframe and save to .csv\n",
    "    frame_id, speaker, listener = [],[],[]\n",
    "    frame_id = list(df['frame'].unique().astype(int))\n",
    "    spk,lsn = [],[]\n",
    "    for j in range(len(df)):\n",
    "        if j>0 and df['frame'].iloc[j]!=df['frame'].iloc[j-1]: # whenever a new frame is coming\n",
    "            speaker.append(spk)\n",
    "            listener.append(lsn)\n",
    "            spk,lsn = [],[]\n",
    "        if not np.isnan(df['track_id'].iloc[j]):\n",
    "            if df['ASDscore'].iloc[j]>0:\n",
    "                spk.append(df['track_id'].iloc[j].astype(int))\n",
    "            else:\n",
    "                lsn.append(df['track_id'].iloc[j].astype(int))\n",
    "        if j==len(df)-1: # the final frame is coming\n",
    "            speaker.append(spk)\n",
    "            listener.append(lsn)\n",
    "            \n",
    "    folder_path = i / output_folder\n",
    "    Path(folder_path).mkdir(exist_ok=True)\n",
    "    pd.DataFrame({'frame':frame_id, 'speaker':speaker, 'listener':listener}).to_csv(folder_path / output_file, index=False)               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7025648-d903-4246-b069-e229bd77b2ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "Remove short silence intervals  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2910a1f6-c6ff-4bc6-83e2-53ac62eb12af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add2dict(dictionary, key, value):\n",
    "    if key not in dictionary:\n",
    "        dictionary[key] = [value]\n",
    "    elif type(dictionary[key]) == list:\n",
    "        dictionary[key].append(value)\n",
    "    else:\n",
    "        dictionary[key] = [dictionary[key], value]\n",
    "\n",
    "def find_utternce_or_silence(speaker, frame_id, mode): #mode='utterance or silence'\n",
    "    d_right, d_left={},{}\n",
    "    for j in range(1, len(frame_id)):\n",
    "        right = list(set(speaker[j-1])-set(speaker[j])) # the speaker exists in previous frame but not exists in current frame\n",
    "        for item in right:\n",
    "            add2dict(d_right, key=item, value=j-1)\n",
    "        left = list(set(speaker[j])-set(speaker[j-1]))\n",
    "        for item in left:\n",
    "            add2dict(d_left, key=item, value=j)\n",
    "        # d_left is {speaker_id:[left endpoint]}, d_right is {speaker_id:[right endpoint]}\n",
    "\n",
    "    d={}\n",
    "    if mode=='utterance': # return {speaker_id:[left_endpoint, right_endpoint]}, which means utterance interval\n",
    "        for key in d_right:\n",
    "            if key in d_left:\n",
    "                if d_left[key][0]<=d_right[key][0]:\n",
    "                    d[key] = list(zip(d_left[key], d_right[key]))\n",
    "                elif len(d_right[key])>1 and d_left[key][0]<=d_right[key][1]:\n",
    "                    d[key] = list(zip(d_left[key], d_right[key][1:]))\n",
    "    elif mode=='silence': # return {speaker_id:[right_endpoint, next_left_endpoint]}, which means silence interval\n",
    "        for key in d_right:\n",
    "            if key in d_left:\n",
    "                if d_right[key][0]<d_left[key][0]:\n",
    "                    d[key] = list(zip(d_right[key], d_left[key]))\n",
    "                elif len(d_left[key])>1 and d_right[key][0]<d_left[key][1]:\n",
    "                    d[key] = list(zip(d_right[key], d_left[key][1:]))\n",
    "    else:\n",
    "        print(\"mode is not correct, please check!\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826f145f-5fcb-4682-93cd-1524a3b450d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = \"turn.csv\"\n",
    "output_file = \"turn_PP1.csv\"\n",
    "        \n",
    "if PP1:\n",
    "    for i in subfolder:\n",
    "\n",
    "        df=pd.read_csv(i / output_folder / input_file)\n",
    "        speaker = df['speaker'].apply(lambda x: literal_eval(str(x))).tolist()\n",
    "        listener = df['listener'].apply(lambda x: literal_eval(str(x))).tolist()\n",
    "        frame_id = df['frame'].tolist()\n",
    "\n",
    "        d=find_utternce_or_silence(speaker, frame_id, mode='silence')\n",
    "        # now d is {speaker_id:[(right-endpoint, left-endpoint),(),...]}\n",
    "        # we just need to check the length of each right-left, which is the silence interval, and fill the short ones\n",
    "        for key in d:\n",
    "            for right, left in d[key]:\n",
    "                assert right<left\n",
    "                if frame_id[left]-frame_id[right]-1 < PP1_th:\n",
    "                    for j in range(right+1, left):\n",
    "                        speaker[j]+=[key]\n",
    "                        if key in listener[j]:\n",
    "                            listener[j].remove(key) \n",
    "        df = pd.DataFrame({'frame':frame_id, 'speaker':speaker, 'listener':listener})\n",
    "        df.to_csv(i / output_folder / output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a390be8-d9ae-4dfe-88ed-f9054c01acae",
   "metadata": {},
   "source": [
    "Remove short utterance intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6654e222-91ac-4b9b-9a06-16699bde2cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = \"turn_PP1.csv\"\n",
    "output_file = \"turn_PP2.csv\"\n",
    "\n",
    "if PP2:\n",
    "    for i in subfolder:\n",
    "\n",
    "        df=pd.read_csv(i / output_folder / input_file)\n",
    "        speaker = df['speaker'].apply(lambda x: literal_eval(str(x))).tolist()\n",
    "        listener = df['listener'].apply(lambda x: literal_eval(str(x))).tolist()\n",
    "        frame_id = df['frame'].tolist()\n",
    "\n",
    "        d=find_utternce_or_silence(speaker, frame_id, mode='utterance')\n",
    "        # now d is {speaker_id:[(left-endpoint, right-endpoint),(),...]}\n",
    "        # we just need to check the length of each right-left, which is the utterance interval, and remove the short ones\n",
    "        for key in d:\n",
    "            for left, right in d[key]:\n",
    "                assert left<=right                    \n",
    "                if frame_id[right]+1-frame_id[left] < PP2_th:\n",
    "                    for j in range(left, right+1):\n",
    "                        if key in speaker[j]:\n",
    "                            speaker[j].remove(key)\n",
    "                        listener[j]+=[key]\n",
    "        df = pd.DataFrame({'frame':frame_id, 'speaker':speaker, 'listener':listener})\n",
    "        df.to_csv(i / output_folder / output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8fa39-f784-41b8-b4e3-818ac658f1c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "get training sample information (x_id, x_frame, y_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4119132-eddb-475f-89fd-ab7a3efd3d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = \"turn_PP2.csv\"\n",
    "output_file = \"turn.json\"\n",
    "# output (x_id, x_start, x_end, y_id, y_start, y_end), which is necessary information for a training sample\n",
    "\n",
    "if PP3:\n",
    "    for i in subfolder:   # find the position of x,y\n",
    "        df=pd.read_csv(i / output_folder / input_file)\n",
    "        speaker = df['speaker'].apply(lambda x: literal_eval(str(x))).tolist()\n",
    "        listener = df['listener'].apply(lambda x: literal_eval(str(x))).tolist()\n",
    "        frame_id = df['frame'].tolist()\n",
    "#         d = dict(zip(frame_id, zip(speaker, listener))    \n",
    "#         df = pd.DataFrame(speaker, index=frame_id, columns =['speaker_id'])\n",
    "        \n",
    "        d=find_utternce_or_silence(speaker, frame_id, mode='utterance')\n",
    "        # now d is {speaker_id:[(left-endpoint, right-endpoint),(),...]}\n",
    "        # we can assign y_id=speaker_id, y_start=left_endpoint, y_end=right_endpoint, \n",
    "        # x_start=left_endpoint-LEN_th, x_end=left_endpoint-1, x_id=listener in [x_start:x_end+1],\n",
    "        training_sample=[]\n",
    "        for key in d:\n",
    "            for left, right in d[key]:\n",
    "                y_id = key\n",
    "                y_start = left # this is index, not frame number, the same below. \n",
    "                y_end = right\n",
    "                x_start = left-LEN_th_max if left-LEN_th_max >= 0 else 0\n",
    "                x_end = left-1 if left-1 >= 0 else 0\n",
    "                \n",
    "                if LEN_exclude == 'silence':\n",
    "                    while x_end > x_start and speaker[x_end]==[]:\n",
    "                        x_end -= 1\n",
    "                else:\n",
    "                    x_end = max(x_start, x_end - LEN_exclude)\n",
    "                    \n",
    "                x_id = list(set().union(*listener[x_start:x_end+1]))\n",
    "                for x in x_id:\n",
    "                    x_mask = [x in j for j in listener[x_start:x_end+1]]*np.arange(x_start,x_end+1)\n",
    "                    x_frame = x_mask[x_mask!=0]\n",
    "                    x_frame = np.array(frame_id)[x_frame] # convert to frame number\n",
    "                    if len(x_frame) >= LEN_th_min:\n",
    "                        training_sample.append({\"x_id\":x, \"x_frame\":list(map(int, x_frame)), \"y_id\":y_id})\n",
    "                 \n",
    "        with open(i / output_folder / output_file,'w') as fp:\n",
    "            json.dump(training_sample, fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33a602-9b37-43d1-b3a9-8f1169673116",
   "metadata": {},
   "source": [
    "Do row (frame) selection and column (feature) selection, and output data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9273491-b51c-4bd4-9f9c-968a6656b4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing..\\data\\train\\train_vimeo_562725349\n",
      "finish ..\\data\\train\\train_vimeo_562725349\n",
      "start processing..\\data\\train\\train_vimeo_611083383\n",
      "finish ..\\data\\train\\train_vimeo_611083383\n",
      "start processing..\\data\\test\\test_vimeo_414944373\n",
      "finish ..\\data\\test\\test_vimeo_414944373\n",
      "start processing..\\data\\val\\val_vimeo_429364395\n",
      "finish ..\\data\\val\\val_vimeo_429364395\n"
     ]
    }
   ],
   "source": [
    "input_file = \"turn.json\"\n",
    "output_file = \"training_sample_detail.json\"\n",
    "asd_file = \"openface_asd_pairs.csv\"\n",
    "data_folder = \"data_sample\"\n",
    "# output (x_id, x_start, x_end, y_id, y_start, y_end), which is necessary information for a training sample\n",
    "\n",
    "if PP4:\n",
    "    for i in subfolder:\n",
    "        print(\"start processing\" + str(i))\n",
    "        stat=[]\n",
    "\n",
    "        asd = pd.read_csv(i / asd_file)\n",
    "        \n",
    "        columns = list(asd.columns) \n",
    "        ind = [columns.index(i) for i in output_col]\n",
    "        \n",
    "        with open(i / output_folder / input_file, \"r\") as fp:\n",
    "            samples = json.load(fp)  # [x_id, x_frame, y_id], id is related to ASD \n",
    "                   \n",
    "        shutil.rmtree(i / output_folder / data_folder)\n",
    "        Path(i / output_folder / data_folder).mkdir(exist_ok=True)\n",
    "        for k in range(len(samples)):\n",
    "            x_id, x_frame, y_id = samples[k][\"x_id\"], samples[k][\"x_frame\"], samples[k][\"y_id\"]\n",
    "            result = asd.loc[((asd['track_id']==x_id) & (asd['frame'].isin(x_frame))), info_col+output_col]\n",
    "                             \n",
    "            # make sure the result is in ascendence order of 'frame'\n",
    "            result = result.sort_values('frame')\n",
    "            \n",
    "            # check if there are NaNs in the result (which means openface failed to extract features), can do interpolation or just discard\n",
    "            if InterpolateNaN:\n",
    "                result.interpolate(inplace=True, limit_direction='both') # must have limit_direction='both'\n",
    "            else:\n",
    "                result.dropna(inplace=True)\n",
    "\n",
    "            stat.append({\"filename\":\"data_\"+str(k), \"x_id\":x_id, \"y_id\":y_id, \"x_frame\":result['frame'].values.tolist(),\n",
    "                         \"x_pos\":result[['bbox_xmin','bbox_ymin','bbox_xmax','bbox_ymax']].values.tolist()})\n",
    "            data = np.float32(result.iloc[:,len(info_col):])\n",
    "            output=(x_id, y_id, data)\n",
    "            with open(i / output_folder/ data_folder / (\"data_\"+str(k)), \"wb\") as fp:\n",
    "                pickle.dump(output, fp)\n",
    "        with open(i / output_folder / output_file, 'w') as fp:\n",
    "            json.dump(stat, fp)      \n",
    "        print(\"finish \"+str(i))   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a38fa2-e5be-4d95-a7d3-beef812d309a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
